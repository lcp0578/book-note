## 第2章 词元和嵌入
- 语言模型处理文本时会将其分成小块，称为词元。为了理解自然语言，语言模型需要将词元转换为数值表示，即嵌入向量。
- 在模型处理文本之前，分词器会将文本分解成词或子词。
- 分词器处理输入的提示词，得到词元ID列表，这就是语言模型的实际输入。
- 流行的分词方法包括字节对编码（BPE，byte pair encoding，广泛用于GPT模型）和WordPiece（用于BERT模型）。

- BERT
	- BERT，全称为Bidirectional Encoder Representations from Transformers，是由Google AI Language团队在2018年提出的预训练语言模型。BERT是基于Transformer网络架构和预训练语言模型的思想而提出的。它可以在不同语言任务上达到最先进的水平。
	- BERT展示了预训练语言模型对于自然语言理解任务的巨大潜力，在诸多任务中取得了突破性进展，成为了自然语言理解任务中的基准模型。
	- BERT的训练过程分为预训练和微调两部分。
		- 预训练是BERT模型的基础部分，它包括使用大量的文本来训练语言模型。在预训练阶段，BERT模型会学习到大量的语言知识，如词汇、语法、句子结构等。预训练的目的是为了让BERT模型具有足够的语言能力来处理各种不同的自然语言任务。
		- 微调过程是在预训练模型的基础上，使用更小的标记数据来调整模型参数。这样可以使得模型更适合特定的任务。大部分使用BERT技术来装备NLP能力的企业，只需要通过微调来让模型更适合特定的任务，而不需要重新预训练。 而预训练过程需要大量的计算资源和时间，所以微调是一种更加高效和经济的方式。
	- BERT主要用于自然语言理解，具体应用如下：
		- 问答系统：BERT可以在问答系统中用来理解问题并生成答案。
		- 句子相似度比较：BERT可以用来比较两个句子之间的相似程度。
		- 文本分类：BERT可以用来对文本进行分类。
		- 情感分析：BERT可以用来对文本进行情感分析。
		- 命名实体识别：BERT可以用来识别文本中的命名实体。

- GPT
	- GPT（Generative Pre-trained Transformer）则是由OpenAI团队在2018年提出的一种语言模型。其起源于对传统预训练语言模型（如ELMO和ULMFit）的改进和升级，采用了Transformer架构，并通过预训练+微调的方式实现语言理解和生成。
	- GPT展示了预训练语言模型在语言生成任务中的潜力。它被广泛应用于各种文本生成任务，如文本自动完成、对话生成、文章摘要等。
	- GPT预训练的数据来源是网络上的大量文本数据，例如维基百科，新闻文章等。模型首先学习了基本的语言知识和结构，然后再在特定的任务上进行微调。微调过程中，模型会根据特定任务的需要来学习相关的知识。
	- GPT能够完成各种自然语言处理任务，在文本生成方面表现尤为优秀，可以生成各种类型的文本，如文章、诗歌、对话等。其主要具体应用如下：
		- 文本生成：GPT可以用来生成文本。
		- 文本自动完成：GPT可以用来自动完成用户输入的文本。
		- 语言翻译：GPT可以用来生成翻译后的文本。
		- 对话生成: GPT可以用来生成对话
		- 摘要生成: GPT可以用来生成文章摘要

- 四种主要的分词方式
	- 词级分词
	- 子词级分词
	- 字符级分词
	- 字节级分词
- 分词器中出现的词元是由三个主要因素决定的：分词方法，用于初始化分词器的参数和特殊词元，以及用于训练分词器的数据集。
- 分词器属性
	- 分词方法：分词方法有许多种，其中BPE是最流行的一种。每种方法都定义了一种算法，用于选择合适的词元集来表示数据集。
	- 用于初始化分词器的参数：选择分析方法后，LLM设计者需要设置分词器的一些关键参数，主要包括词表大小、特殊词元和大小写处理策略。
	- 数据领域：即使我们选择相同的方法和参数，分词器的行为也会因其训练所用的数据集而有所不同。
- 词元嵌入
	- 词元嵌入是用于捕捉语言中含义和模式的数值表示空间。
- 分析器经过初始化和训练，就会在其关联的语言模型的训练过程中使用。
- 与使用静态向量表示每个词元或词不同，语言模型会创建于**上下文相关**（centextualized）的词嵌入。所谓上下文相关，就是根据词元在上下文中的含义使用不同的表示方式。
- 语言模型能够生成高质量与上下文相关的词元嵌入，这种嵌入改进了原始的静态嵌入。这些与上下文相关的词元嵌入可以用于命名实体识别、抽取式文本摘要和文本分类等任务。除了生成词元嵌入，语言模型还可以生成覆盖整个句子甚至文档的文本嵌入。