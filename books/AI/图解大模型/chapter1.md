## 第1章 大语言模型简介
- GPT （Generative Pre-trained Transformer），中文全称是“生成式预训练变换器”。
- **语言人工智能**是人工智能的一个子领域，专注于开发能够理解、处理和生成人类语言的技术。随着机器学习方法在解决语言处理问题方面取得持续成功，语言人工智能这个术语与**自然语言处理**(natural language processing, NLP)经常可以互换使用。
- 词袋模型的第一步是**分词**(tokenization)，即将句子拆分成单个词或子词（词元，token）。
- word2vec（词向量）于2013年发布，是首批成功利用**嵌入**(embedding)这个概念来捕捉文本含义的技术之一。嵌入是数据的向量表示，试图捕捉数据的含义。为此，word2vec通过在大量文本数据上训练来学习词的语义表示。
	- 为了生成这些语义表示，word2vec利用了**神经网络**（neural netword）技术。神经网络由处理信息的多层互连节点组成。神经网络可以有多个“层”，每个链接都有一定的权重，这些权重通常被称为模型的**参数**。
- 嵌入的类型
	- 有许多类型的嵌入，如词嵌入和句子嵌入，它们用于表示不同层次的抽象（词与句子）。
- 使用注意力机制编解码上下文
	- 注意力允许模型关注输入序列中彼此相关（相互“注意”）的部分，并放大它们的信息。注意力机制通过选择性地聚焦于句子中最关键的词，来突出其重要性。
- 在Transformer中，编码和解码的组件相互堆叠。这种架构仍然是自回归的，每个新生成的词都被模型用于生成下一个词。
	- 编码器、解码器这些模块共同构成了Transformer架构，是语言人工智能中许多影响深远的模型（如BERT和GPT-1）的基础。
- BERT类模型通常用于**迁移学习**（transfer learning），这包括首先针对语言建模进行**预训练**（pretraining），然后针对特定任务进行**微调**（fine-tuning）。
- 生成模型：仅解码器模型
- 表示模型：仅编码器模型
- LLM的训练范式
	- 创建LLM通常包含至少两步骤：
		- 第一步称为**预训练**，占用了创建LLM过程中的大部分算力和训练时间。LLM在海量互联网文本语料库上进行训练，使模型能够学习语法、上下文和语言模式。这个宽泛的训练阶段并不是针对特定的任务或应用的，而仅仅用于预测下一个词。由此产生的模型通常被称为基础模型或基座模型。这些模型通常不会遵循指令。
		- 第二步是**微调**，有时称为**后训练**（post-training），包括使用先前训练好的模型，并在更具体的任务上进行进一步训练。这使得LLM能够适应特定任务或展现符合人们期望的行为。
- 专有模型
	- 专有模型是闭源模型的一种。闭源LLM是指不想公众公开其权重和架构的模型。
- 开源模型
	- 开源LLM是指向公众共享其权重和架构的模型。它们仍然由特定组织开发，但通常会共享用于创建或者本地运行模型的代码。